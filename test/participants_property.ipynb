{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle bugs in some docker container: 'pip install -e .'  doesn't work \n",
    "from IPython.display import clear_output\n",
    "try:\n",
    "    import Flamingo\n",
    "except ModuleNotFoundError:\n",
    "    import sys \n",
    "    sys.path.append('..')\n",
    "    import Flamingo\n",
    "lang_encoder_path = \"facebook/opt-125m\"\n",
    "tokenizer_path = lang_encoder_path\n",
    "cache_dir = None \n",
    "model_config = dict(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=lang_encoder_path,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    cross_attn_every_n_layers=1,\n",
    "    cache_dir = cache_dir,\n",
    "    lora_tuning=False,\n",
    "    decoupled=True  \n",
    ")\n",
    "\n",
    "import torch\n",
    "from Flamingo.lora_tuning import create_model_and_transforms \n",
    "from Flamingo.models.batchprocessor import DecoupledFlamingoBatchProcessor\n",
    "from Flamingo.datasets.gtsrb import classes\n",
    "from Flamingo.config.participants_property import dataset_config\n",
    "from Flamingo.datasets import build_dataset\n",
    "from Flamingo.inference.vis import show_pred_with_gt\n",
    "from Flamingo.inference import post_process\n",
    "import pandas as pd \n",
    "import re\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model, image processor and tokenizer\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    **model_config\n",
    ")\n",
    "print(\"Load state dict:\")\n",
    "state_dict = torch.load(\"/root/yunzhi/flamingo_retrieval/retrieval/work_dir/99/weight.pth\")\n",
    "keys1 = model.lang_encoder.gated_cross_attn_layers.load_state_dict(state_dict, strict=False)\n",
    "keys2 = model.perceiver.load_state_dict(state_dict, strict=False)\n",
    "dataset = build_dataset(\n",
    "    dataset_config=dataset_config,\n",
    "    vis_processor=image_processor,\n",
    "    tokenizer=tokenizer)\n",
    "model = model.to(device=\"cuda:2\", dtype=torch.bfloat16)\n",
    "\n",
    "batch_processor = DecoupledFlamingoBatchProcessor(cast_type='bf16', tokenizer=tokenizer)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vision_x',\n",
       " 'category_name',\n",
       " 'attributes_name',\n",
       " 'ori_img_name',\n",
       " 'file_name',\n",
       " 'bbox',\n",
       " 'area',\n",
       " 'pth_file']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in dataset[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([110.2237445779, 294.2411995329, 81.59200867539998, 116.00141808469999],\n",
       " 'pedestrian',\n",
       " '0-50 obstruction',\n",
       " torch.Size([1, 1, 1, 256, 1024]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    bbox = data['bbox']\n",
    "    x, y, w, h = bbox\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    if x <0 or y < 0 or w < 0 or h < 0:\n",
    "        continue \n",
    "    break \n",
    "data['bbox'], data['category_name'], data['attributes_name'], data['vision_x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before post process: ['Output:50-80 obstruction motorcycle electric vehicle (BEV),” the motorcycle electric vehicle (BEV']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Output:50-80 obstruction motorcycle electric vehicle (BEV),” the motorcycle electric vehicle (BEV']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    output = batch_processor(model=model, batch=data['vision_x'], mode='test',\n",
    "            text_prompt=\"<image>Output:\", num_beams=3, max_new_tokens=20)\n",
    "    print(\"before post process:\", output)\n",
    "    # output = post_process(output, cats=cats)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vision_x': tensor([[[[[-0.6464, -1.5234, -0.2136,  ...,  0.7257, -0.1189, -0.2867],\n",
       "            [ 0.4762,  0.5485,  0.0902,  ..., -0.1007,  0.1825, -0.4409],\n",
       "            [ 0.5197,  0.9373,  0.1619,  ..., -0.0025,  0.5712, -0.3488],\n",
       "            ...,\n",
       "            [ 0.7610,  0.9900,  0.4876,  ...,  0.8991,  0.3367,  0.4553],\n",
       "            [ 0.2455,  1.8385, -0.3149,  ...,  1.2226, -0.5920,  0.3794],\n",
       "            [ 0.6058, -0.0835,  0.1039,  ...,  0.3892, -0.0593,  0.2578]]]]]),\n",
       " 'category_name': 'pedestrian',\n",
       " 'attributes_name': '0-50 obstruction',\n",
       " 'ori_img_name': '075e8be308be5bd67db1251ffd335134.jpg',\n",
       " 'file_name': '075e8be308be5bd67db1251ffd335134.jpg_1.pth',\n",
       " 'bbox': [110.2237445779,\n",
       "  294.2411995329,\n",
       "  81.59200867539998,\n",
       "  116.00141808469999],\n",
       " 'area': 9464.788710725543,\n",
       " 'pth_file': '/root/datasets/cached_participants_property/pth/pedestrian/075e8be308be5bd67db1251ffd335134.jpg_1.pth'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
