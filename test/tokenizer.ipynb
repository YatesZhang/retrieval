{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'question', 'answer_start', 'answer_end'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")\n",
    "raw_datasets = raw_datasets.remove_columns([\"id\", \"title\"])\n",
    "\n",
    "def prepare_data(example):\n",
    "    answer = example[\"answers\"][\"text\"][0]\n",
    "    example[\"answer_start\"] = example[\"answers\"][\"answer_start\"][0]\n",
    "    example[\"answer_end\"] = example[\"answer_start\"] + len(answer)\n",
    "    return example\n",
    "\n",
    "raw_datasets = raw_datasets.map(prepare_data, remove_columns=[\"answers\"])\n",
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "print(f\"Context: {raw_datasets['train'][0]['context']}\")\n",
    "print(f\"Question: {raw_datasets['train'][0]['question']}\")\n",
    "start = raw_datasets[\"train\"][0][\"answer_start\"]\n",
    "end = raw_datasets[\"train\"][0][\"answer_end\"]\n",
    "print(f\"\\nAnswer: {raw_datasets['train'][0]['context'][start:end]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = raw_datasets[\"train\"][0]\n",
    "inputs = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    truncation=\"only_second\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=384,\n",
    "    stride=128,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Output : [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer(\"Output:\", \"speed limited 30 (km/h)\", return_tensors='pt')\n",
    "input_ids = out['input_ids']\n",
    "token_type_ids = out['token_type_ids']\n",
    "tokenizer.decode(\n",
    "    token_ids=input_ids[token_type_ids==0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_labels(offsets, answer_start, answer_end, sequence_ids):\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully in the context, return (0, 0)\n",
    "    if offsets[context_start][0] > answer_end or offsets[context_end][1] < answer_start:\n",
    "        return (0, 0)\n",
    "    else:\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offsets[idx][0] <= answer_start:\n",
    "            idx += 1\n",
    "        start_position = idx - 1\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offsets[idx][1] >= answer_end:\n",
    "            idx -= 1\n",
    "        end_position = idx + 1\n",
    "    \n",
    "        return start_position, end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['context', 'question', 'answer_start', 'answer_end'], 515)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in example], example['answer_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = find_labels(\n",
    "    inputs[\"offset_mapping\"][0],\n",
    "    example[\"answer_start\"],\n",
    "    example[\"answer_end\"],\n",
    "    inputs.sequence_ids(0)\n",
    ")\n",
    "tokenizer.decode(inputs[\"input_ids\"][0][start: end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0),\n",
       "  (0, 2),\n",
       "  (3, 7),\n",
       "  (8, 11),\n",
       "  (12, 15),\n",
       "  (16, 22),\n",
       "  (23, 27),\n",
       "  (28, 37),\n",
       "  (38, 44),\n",
       "  (45, 47),\n",
       "  (48, 52),\n",
       "  (53, 55),\n",
       "  (56, 59),\n",
       "  (59, 63),\n",
       "  (64, 70),\n",
       "  (70, 71),\n",
       "  (0, 0),\n",
       "  (0, 13),\n",
       "  (13, 15),\n",
       "  (15, 16),\n",
       "  (17, 20),\n",
       "  (21, 27),\n",
       "  (28, 31),\n",
       "  (32, 33),\n",
       "  (34, 42),\n",
       "  (43, 52),\n",
       "  (52, 53),\n",
       "  (54, 56),\n",
       "  (56, 58),\n",
       "  (59, 62),\n",
       "  (63, 67),\n",
       "  (68, 76),\n",
       "  (76, 77),\n",
       "  (77, 78),\n",
       "  (79, 83),\n",
       "  (84, 88),\n",
       "  (89, 91),\n",
       "  (92, 93),\n",
       "  (94, 100),\n",
       "  (101, 107),\n",
       "  (108, 110),\n",
       "  (111, 114),\n",
       "  (115, 121),\n",
       "  (122, 126),\n",
       "  (126, 127),\n",
       "  (128, 139),\n",
       "  (140, 142),\n",
       "  (143, 148),\n",
       "  (149, 151),\n",
       "  (152, 155),\n",
       "  (156, 160),\n",
       "  (161, 169),\n",
       "  (170, 173),\n",
       "  (174, 180),\n",
       "  (181, 183),\n",
       "  (183, 184),\n",
       "  (185, 187),\n",
       "  (188, 189),\n",
       "  (190, 196),\n",
       "  (197, 203),\n",
       "  (204, 206),\n",
       "  (207, 213),\n",
       "  (214, 218),\n",
       "  (219, 223),\n",
       "  (224, 226),\n",
       "  (226, 229),\n",
       "  (229, 232),\n",
       "  (233, 237),\n",
       "  (238, 241),\n",
       "  (242, 248),\n",
       "  (249, 250),\n",
       "  (250, 251),\n",
       "  (251, 254),\n",
       "  (254, 256),\n",
       "  (257, 259),\n",
       "  (260, 262),\n",
       "  (263, 264),\n",
       "  (264, 265),\n",
       "  (265, 268),\n",
       "  (268, 269),\n",
       "  (269, 270),\n",
       "  (271, 275),\n",
       "  (276, 278),\n",
       "  (279, 282),\n",
       "  (283, 287),\n",
       "  (288, 296),\n",
       "  (297, 299),\n",
       "  (300, 303),\n",
       "  (304, 312),\n",
       "  (313, 315),\n",
       "  (316, 319),\n",
       "  (320, 326),\n",
       "  (327, 332),\n",
       "  (332, 333),\n",
       "  (334, 345),\n",
       "  (346, 352),\n",
       "  (353, 356),\n",
       "  (357, 358),\n",
       "  (358, 361),\n",
       "  (361, 365),\n",
       "  (366, 368),\n",
       "  (369, 372),\n",
       "  (373, 374),\n",
       "  (374, 377),\n",
       "  (377, 379),\n",
       "  (379, 380),\n",
       "  (381, 382),\n",
       "  (383, 389),\n",
       "  (390, 395),\n",
       "  (396, 398),\n",
       "  (399, 405),\n",
       "  (406, 409),\n",
       "  (410, 420),\n",
       "  (420, 421),\n",
       "  (422, 424),\n",
       "  (425, 427),\n",
       "  (428, 429),\n",
       "  (430, 437),\n",
       "  (438, 440),\n",
       "  (441, 444),\n",
       "  (445, 446),\n",
       "  (446, 449),\n",
       "  (449, 451),\n",
       "  (452, 454),\n",
       "  (455, 458),\n",
       "  (458, 462),\n",
       "  (462, 463),\n",
       "  (464, 470),\n",
       "  (471, 476),\n",
       "  (477, 480),\n",
       "  (481, 487),\n",
       "  (488, 492),\n",
       "  (493, 500),\n",
       "  (500, 502),\n",
       "  (503, 511),\n",
       "  (512, 514),\n",
       "  (515, 520),\n",
       "  (521, 525),\n",
       "  (525, 528),\n",
       "  (528, 531),\n",
       "  (532, 534),\n",
       "  (534, 537),\n",
       "  (537, 541),\n",
       "  (542, 544),\n",
       "  (545, 549),\n",
       "  (549, 550),\n",
       "  (551, 553),\n",
       "  (554, 557),\n",
       "  (558, 561),\n",
       "  (562, 564),\n",
       "  (565, 568),\n",
       "  (569, 573),\n",
       "  (574, 579),\n",
       "  (580, 581),\n",
       "  (581, 584),\n",
       "  (585, 587),\n",
       "  (588, 589),\n",
       "  (590, 596),\n",
       "  (597, 601),\n",
       "  (602, 606),\n",
       "  (607, 615),\n",
       "  (616, 623),\n",
       "  (624, 625),\n",
       "  (626, 633),\n",
       "  (634, 637),\n",
       "  (638, 641),\n",
       "  (642, 646),\n",
       "  (647, 651),\n",
       "  (651, 652),\n",
       "  (652, 653),\n",
       "  (654, 656),\n",
       "  (657, 658),\n",
       "  (659, 665),\n",
       "  (665, 666),\n",
       "  (667, 673),\n",
       "  (674, 679),\n",
       "  (680, 686),\n",
       "  (687, 689),\n",
       "  (690, 694),\n",
       "  (694, 695),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    \n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    inputs[\"start_positions\"] = []\n",
    "    inputs[\"end_positions\"] = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        start, end = find_labels(\n",
    "            offset, examples[\"answer_start\"][sample_idx], examples[\"answer_end\"][sample_idx], inputs.sequence_ids(i)\n",
    "        )\n",
    "        \n",
    "        inputs[\"start_positions\"].append(start)\n",
    "        inputs[\"end_positions\"].append(end)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319234a5cba44dc3bd9dc346018876fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3372e762e04a4018b5056695ce4835b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'start_positions',\n",
       " 'end_positions']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in tokenized_datasets['train'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenized_datasets['train'][0]\n",
    "tokenizer.decode(data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \" Venite Ad Me Omnes \". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i, mask in enumerate(data['token_type_ids']):\n",
    "    if mask == 1:\n",
    "        res.append(data['input_ids'][i])\n",
    "tokenizer.decode(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 101, 20164, 10932, 2271, 7954, 1110, 1359, 1107, 17520, 102, 2777, 1110, 20164, 10932, 2271, 7954, 1359, 136, 102, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = '[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]'\n",
    "tokenizer(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Flamingo.lora_tuning import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>Speed limit (20km/h)<pad><pad>\n",
      "</s>Speed limit (30km/h)<pad><pad>\n",
      "</s>Speed limit (50km/h)<pad><pad>\n",
      "</s>Speed limit (60km/h)<pad><pad>\n",
      "</s>Speed limit (70km/h)<pad><pad>\n",
      "</s>Speed limit (80km/h)<pad><pad>\n",
      "</s>End of speed limit (80km/h)\n",
      "</s>Speed limit (100km/h)<pad><pad>\n",
      "</s>Speed limit (120km/h)<pad><pad>\n",
      "</s>No passing<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>No passing veh over 3.5 tons<pad><pad>\n",
      "</s>Right-of-way at intersection<pad><pad><pad>\n",
      "</s>Priority road<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Yield<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Stop<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>No vehicles<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Veh > 3.5 tons prohibited<pad><pad><pad>\n",
      "</s>No entry<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>General caution<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Dangerous curve left<pad><pad><pad><pad><pad>\n",
      "</s>Dangerous curve right<pad><pad><pad><pad><pad>\n",
      "</s>Double curve<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Bumpy road<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Slippery road<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Road narrows on the right<pad><pad><pad><pad>\n",
      "</s>Road work<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Traffic signals<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Pedestrians<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Children crossing<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Bicycles crossing<pad><pad><pad><pad><pad><pad>\n",
      "</s>Beware of ice/snow<pad><pad><pad>\n",
      "</s>Wild animals crossing<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>End speed + passing limits<pad><pad><pad><pad><pad>\n",
      "</s>Turn right ahead<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Turn left ahead<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Ahead only<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Go straight or right<pad><pad><pad><pad><pad><pad>\n",
      "</s>Go straight or left<pad><pad><pad><pad><pad><pad>\n",
      "</s>Keep right<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Keep left<pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>Roundabout mandatory<pad><pad><pad><pad><pad><pad><pad>\n",
      "</s>End of no passing<pad><pad><pad><pad><pad><pad>\n",
      "</s>End no passing veh > 3.5 tons<pad>\n"
     ]
    }
   ],
   "source": [
    "classes = { 0:'Speed limit (20km/h)',\n",
    "            1:'Speed limit (30km/h)', \n",
    "            2:'Speed limit (50km/h)', \n",
    "            3:'Speed limit (60km/h)', \n",
    "            4:'Speed limit (70km/h)', \n",
    "            5:'Speed limit (80km/h)', \n",
    "            6:'End of speed limit (80km/h)', \n",
    "            7:'Speed limit (100km/h)', \n",
    "            8:'Speed limit (120km/h)', \n",
    "            9:'No passing', \n",
    "            10:'No passing veh over 3.5 tons', \n",
    "            11:'Right-of-way at intersection', \n",
    "            12:'Priority road', \n",
    "            13:'Yield', \n",
    "            14:'Stop', \n",
    "            15:'No vehicles', \n",
    "            16:'Veh > 3.5 tons prohibited', \n",
    "            17:'No entry', \n",
    "            18:'General caution', \n",
    "            19:'Dangerous curve left', \n",
    "            20:'Dangerous curve right', \n",
    "            21:'Double curve', \n",
    "            22:'Bumpy road', \n",
    "            23:'Slippery road', \n",
    "            24:'Road narrows on the right', \n",
    "            25:'Road work', \n",
    "            26:'Traffic signals', \n",
    "            27:'Pedestrians', \n",
    "            28:'Children crossing', \n",
    "            29:'Bicycles crossing', \n",
    "            30:'Beware of ice/snow',\n",
    "            31:'Wild animals crossing', \n",
    "            32:'End speed + passing limits', \n",
    "            33:'Turn right ahead', \n",
    "            34:'Turn left ahead', \n",
    "            35:'Ahead only', \n",
    "            36:'Go straight or right', \n",
    "            37:'Go straight or left', \n",
    "            38:'Keep right', \n",
    "            39:'Keep left', \n",
    "            40:'Roundabout mandatory', \n",
    "            41:'End of no passing', \n",
    "            42:'End no passing veh > 3.5 tons' }\n",
    "for i in classes:\n",
    "    cls = classes[i]\n",
    "    out = tokenizer(cls, max_length=10, padding=\"max_length\", truncation=True)\n",
    "    print(tokenizer.decode(out['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"</s>'Right-of-way at intersection'<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tokenizer(\"'Right-of-way at intersection'\", max_length=20, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "tokenizer.batch_decode(out['input_ids'])\n",
    "# out['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<image>hloow<|endofchunk|></s>', '<image>sort<|endofchunk|></s>'],\n",
       " {'input_ids': tensor([[    2,   108, 13984,    12,  1116,    12,  1970,    23,  8088,   108,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [\"hloow\", \"sort\"]\n",
    "sample = [\n",
    "    (f\"<image>{s.strip()}<|endofchunk|>{tokenizer.eos_token}\") for s in sample\n",
    "]\n",
    "sample, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50266,\n",
       " {'input_ids': [2, 50266, 298, 4082, 1722, 50265, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]},\n",
       " 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]})\n",
    "tokenizer.encode(\"<image>\")[-1], tokenizer(\"<image>hloow<|endofchunk|></s>\"), tokenizer.pad_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
