{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-02 13:53:05,799] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# from Flamingo.utils import pretty_print\n",
    "import pdb \n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image \n",
    "from rich import print \n",
    "import numpy as np \n",
    "from Flamingo.lora_tuning import get_tokenizer\n",
    "from Flamingo.config.baseline import model_config\n",
    "def preprocess_laion_text(sample, tokenizer, max_tokens=32):\n",
    "    \"\"\"\n",
    "    Preprocess text for LAION.\n",
    "    Captions are truncated to 32 tokens by default.\n",
    "    \"\"\"\n",
    "    sample = [\n",
    "        (f\"<image>{s.strip()}<|endofchunk|>{tokenizer.eos_token}\") for s in sample\n",
    "    ]\n",
    "    text = tokenizer(\n",
    "        sample,\n",
    "        max_length=max_tokens,\n",
    "        padding=\"longest\",\n",
    "        truncation=\"only_first\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return text[\"input_ids\"], text[\"attention_mask\"]\n",
    "\n",
    "\n",
    "class VQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self, image_dir_path, question_path, annotations_path, is_train, dataset_name\n",
    "    ):\n",
    "        self.questions = json.load(open(question_path, \"r\"))[\"questions\"]\n",
    "        if annotations_path is not None:\n",
    "            self.answers = json.load(open(annotations_path, \"r\"))[\"annotations\"]\n",
    "        else:\n",
    "            self.answers = None\n",
    "        self.image_dir_path = image_dir_path\n",
    "        self.is_train = is_train\n",
    "        self.dataset_name = dataset_name\n",
    "        if self.dataset_name in {\"vqav2\", \"ok_vqa\"}:\n",
    "            self.img_coco_split = self.image_dir_path.strip(\"/\").split(\"/\")[-1]\n",
    "            assert self.img_coco_split in {\"train2014\", \"val2014\", \"test2015\"}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def get_img_path(self, question):\n",
    "        if self.dataset_name in {\"vqav2\", \"ok_vqa\"}:\n",
    "            return os.path.join(\n",
    "                self.image_dir_path,\n",
    "                f\"COCO_{self.img_coco_split}_{question['image_id']:012d}.jpg\"\n",
    "                if self.is_train\n",
    "                else f\"COCO_{self.img_coco_split}_{question['image_id']:012d}.jpg\",\n",
    "            )\n",
    "        elif self.dataset_name == \"vizwiz\":\n",
    "            return os.path.join(self.image_dir_path, question[\"image_id\"])\n",
    "        elif self.dataset_name == \"textvqa\":\n",
    "            return os.path.join(self.image_dir_path, f\"{question['image_id']}.jpg\")\n",
    "        else:\n",
    "            raise Exception(f\"Unknown VQA dataset {self.dataset_name}\")\n",
    "\n",
    "    def collater(self, samples):\n",
    "        \"\"\"\n",
    "            collate function \n",
    "        \"\"\"\n",
    "        question_list, answer_list, input_id_list, attention_mask_list, labels_list = [], [], [], [], []\n",
    "\n",
    "        for sample in samples:\n",
    "            question_list.append(sample[\"instruction\"])\n",
    "            answer_list.append(sample[\"answer\"])\n",
    "            input_id_list.append(sample[\"input_ids\"])\n",
    "            attention_mask_list.append(sample[\"attention_mask\"])\n",
    "            labels_list.append(sample[\"labels\"])\n",
    "\n",
    "        # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n",
    "        # same length to return tensors.\n",
    "        max_label_length = max(len(l) for l in labels_list)\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "        padded_labels = []\n",
    "        for l in labels_list:\n",
    "            remainder = [-100] * (max_label_length - len(l))\n",
    "            if isinstance(l, list):\n",
    "                l = l + remainder if padding_side == \"right\" else remainder + l\n",
    "            elif padding_side == \"right\":\n",
    "                l = np.concatenate([l, remainder]).astype(np.int64)\n",
    "            else:\n",
    "                l = np.concatenate([remainder, l]).astype(np.int64)\n",
    "            padded_labels.append(l)\n",
    "\n",
    "        padded_samples = self.tokenizer.pad(\n",
    "            {\"input_ids\": input_id_list, \"attention_mask\": attention_mask_list, \"labels\": padded_labels},\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        )\n",
    "\n",
    "        labels = padded_samples[\"labels\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        labels[:, 0] = -100\n",
    "        return {\n",
    "            \"input_ids\": padded_samples[\"input_ids\"],\n",
    "            \"attention_mask\": padded_samples[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "            \"instruction\": question_list,\n",
    "            \"answer\": answer_list,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        img_path = self.get_img_path(question)\n",
    "        image = Image.open(img_path)\n",
    "        image.load()\n",
    "        results = {\n",
    "            \"image\": image,\n",
    "            \"question\": question[\"question\"],\n",
    "            \"question_id\": question[\"question_id\"],\n",
    "        }\n",
    "        if self.answers is not None:\n",
    "            answers = self.answers[idx]\n",
    "            results[\"answers\"] = [a[\"answer\"] for a in answers[\"answers\"]]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_path = \"/home/yunzhi/datasets/COCO/train2014/train2014\"\n",
    "question_path = '/home/yunzhi/datasets/COCO/annotations/v2_OpenEnded_mscoco_train2014_questions.json'\n",
    "annoatation_path = \"/home/yunzhi/datasets/COCO/annotations/v2_mscoco_train2014_annotations.json\"\n",
    "# image_dir_path, question_path, annotations_path, is_train, dataset_name\n",
    "anno = json.load(open(annoatation_path, \"r\"))\n",
    "# pdb.set_trace()\n",
    "dataset = VQADataset(image_dir_path=image_dir_path,\n",
    "            question_path=question_path,\n",
    "                annotations_path=annoatation_path,\n",
    "                is_train=True,\n",
    "                dataset_name='vqav2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Flamingo.lora_tuning import create_model_and_transforms \n",
    "from IPython.display import clear_output\n",
    "model_config['lora_tuning'] = False \n",
    "model, image_processor, text_tokenizer = create_model_and_transforms(\n",
    "    **model_config\n",
    ")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image_processor(dataset[0]['image']).repeat(1, 1, 1, 1)\n",
    "img = img.unsqueeze(1).unsqueeze(0)\n",
    "img = img.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1, 3, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pitcher',\n",
       " 'catcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher',\n",
       " 'pitcher']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'netting'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'mesh'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'net'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'catcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'pitcher'</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'orange'</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'no'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'yes'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'net'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'netting'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'mesh'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'net'\u001b[0m, \u001b[32m'net'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'catcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m,\n",
       "        \u001b[32m'pitcher'\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m, \u001b[32m'orange'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m[\u001b[0m\u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'no'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m, \u001b[32m'yes'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(tokenizer_path=model_config['tokenizer_path'], cache_dir=model_config['cache_dir'])\n",
    "question_sample =  [dataset[i]['question'] for i in range(4)]\n",
    "answers = [dataset[i]['answers'] for i in range(4)]\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image>Question:What is this photo taken looking through? Short answer:net<|endofchunk|><image>Question:What position is this man playing? Short answer:pitcher<|endofchunk|><image>Question:What color is the players shirt? Short answer:orange<|endofchunk|><image>Question:Is this man a professional baseball player? Short answer:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\n",
    "for i in range(3):\n",
    "    prompt_qa = f\"<image>Question:{question_sample[i]} Short answer:{answers[i][0]}<|endofchunk|>\"\n",
    "    prompt += prompt_qa\n",
    "prompt += f\"<image>Question:{question_sample[3]} Short answer:\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What color is the players shirt?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What color is the players shirt?\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What color is the players shirt?\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generated text:  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">&lt;image&gt;</span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Question:What color is the players shirt? Short answer: Black. Long answer:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generated text:  \u001b[1;37m<\u001b[0m\u001b[1;37mimage\u001b[0m\u001b[1;37m>\u001b[0m\u001b[37mQuestion:What color is the players shirt? Short answer: Black. Long answer:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "question = question_sample[2] \n",
    "print(question)\n",
    "\n",
    "prompt_qa = f\"<image>Question:{question} Short answer:\"\n",
    "# prompt_imagenet = f\"<image>A photo of \"\n",
    "lang_x = tokenizer(prompt_qa, return_tensors='pt')\n",
    "input_ids = lang_x['input_ids'].cuda()\n",
    "attention_mask = lang_x['attention_mask'].cuda()\n",
    "generated_text = model.generate(\n",
    "    vision_x=img,\n",
    "    lang_x=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=5,\n",
    "    num_beams=5,\n",
    ")\n",
    "print(\"Generated text: \", \"[white]\" + tokenizer.decode(generated_text[0]) + \"[/white]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is this photo taken looking through?',\n",
       " 'What position is this man playing?',\n",
       " 'What color is the players shirt?',\n",
       " 'Is this man a professional baseball player?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sample\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "tokenizer.padding_side = 'left'\n",
    "input_ids, attention_mask = preprocess_laion_text(question_sample, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<image>What is this photo taken looking through?<|endofchunk|><|endoftext|>',\n",
       " '<image>What position is this man playing?<|endofchunk|><|endoftext|><pad>',\n",
       " '<image>What color is the players shirt?<|endofchunk|><|endoftext|><pad>',\n",
       " '<image>Is this man a professional baseball player?<|endofchunk|><|endoftext|>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Prompts. For captioning tasks, we format demonstrations as \n",
    "<image> Output: [caption], replacing [caption] with the ground-truth caption.\n",
    " For VQA, we format examples as <image> Question: [question] Short answer: [answer]. For HatefulMemes,\n",
    " we prompt the model with <image> is an image with: [text] written on it. Is it hateful? Answer: [answer]\n",
    "\n",
    " Decoding parameters. We evaluate captioning and VQA using beam search with 3 beams,\n",
    "   stopping generation at 20 tokens for captioning,\n",
    "    5 tokens for VQA, or whenever the model produces an <|endofchunk|> token.\n",
    "   For HatefulMemes, we compute the log-likelihood of completions “yes” and “no” and answer with the most likely completion\n",
    "\"\"\"\n",
    "tokenizer.eos_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
